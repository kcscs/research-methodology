\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{arrows.meta}
\usepackage{draftwatermark}
\SetWatermarkText{Writing Exercise\\%
	Fictive Results}
\SetWatermarkScale{0.6}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\newcommand{\com}[1]{}
\newcommand{\eqprop}{EP }
\newcommand{\backprop}{BP }

\begin{document}
	
	%\title{A Sample Article Using IEEEtran.cls\\ for IEEE Journals and Transactions}
	\title{EqPropNEAT\\Extending NEAT with equlibrium propagation}
	
	\author{Cs. Karikó, M. Kovács, Zs. Tornai
		%\author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
			% <-this % stops a space
			%\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
			%\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
			\thanks{disclaimer: TODO}
		}
		
		% The paper headers
		\markboth{ELTE Computer Science MSc - Research methodology course}%
		{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}
		
		\IEEEpubid{0000--0000/00\$00.00~\copyright~2022 IEEE}
		% Remember, if you use this you must call \IEEEpubidadjcol in the second
		% column for its text to clear the IEEEpubid mark.
		
		\maketitle
		
		\begin{abstract}
			Artificial neural networks have been employed successfully to solve a great variety of tasks over the past decade. In most applications, network architecture is usually decided with a trial-and-error process, relying on empirical experience.
			The neuoroevolution of augmenting topologies algorithm (NEAT) \cite{neat} proposes a solution to finding well fitting network architectures to a given task, however for learning weights, it only uses a simple genetic mutation rule.
			
			This paper proposes an extension of NEAT with an additional learning step employing equilibrium propagation \cite{eqprop} for learning edge weights. We claim that the resulting novel technique needs less iterations for convergence, as weights are adjusted by a method which was specifically invented for such task. Our experimental results show a significant reduction in training times on common neural network benchmark tasks, while achieving similar prediction quality.
		\end{abstract}
		
		% \begin{abstract}
			% This document describes the most common article elements and how to use the IEEEtran class with \LaTeX \ to produce files that are suitable for submission to the IEEE.  IEEEtran can produce conference, journal, and technical note (correspondence) papers with a suitable choice of class options. 
			% \end{abstract}
		
		% Opposed to the standard backpropagation approach, equlibrium propagation designed for easy and efficient hardware implementation enabling mich higher performance and faster iterations, which are even more important in the case of genetic optimization techniques.
		
		\begin{IEEEkeywords}
			neuroevolution, neat, equilibrium propagation.
		\end{IEEEkeywords}
		
		\section{Introduction}
		\IEEEPARstart{T}{he} process of designing neural network models always involves deciding on a particular network architecture. In order to avoid both overfitting as well as subpar prediction quality, the right amount of network complexity has to be found for a particular learning task. Over the years, a great amount of experience has accumulated regarding this decision, however these rules are more about empirical know-how, than proven theorems. For our experiment we took well tested and proven algorithms NEAT and Equlibrium Propagation(EP)\cite{eqprop} and tried to design a neural network which best represents a biological solution to a given task. For this reason EP was chosen rather than Backpropagation(BP)\cite{backprop} to further fine tune the network in a way which would be plausible in a biological network. Due to the similarities between BP and EP we hope to capture all the positive aspects of BP while still maintaining the properties of a biological network. Due to the nature of EP our networks learns slower than a comparable network would utilizing BP, but as BP is considered "biologically implausible" we accept this shortcoming. In \textit{Performance} chapter we will discuss the performance compared to BP and we will explain why the difference in real-world applications would not be significant.
		
		\section{Background}
		As mentioned in the previous chapter, the NEAT algorithm uses a simple genetic mutation rule for evolving edge weights. With a sufficiently large population this method can evolve correct connection weights, however this process usually requires lot of generations. A number of extensions have already been proposed for improving weight search by incorporating BP to the original algorithm \cite{neatsurvey}. Such attempts include Learning-NEAT \cite{lneat}, DeepNeat and CoDeepNeat \cite{codeepneat}. A common property of these techniques is that s limited number of training epochs are run during evaluation for adjusting edge weights and determining network performance. 
		
		Equilibrium propagation proposes a new approach to training neural networks. Unlike conventional artificial neural network models, equilibrium propagation models the network as a continuous-time dynamic system. Training and evaluation is accomplished with a numerical simulation of changing energy states converging to an equilibrium. After the simulation has arrived at a fixed point, the results can be read from the state of output nodes. Training is achieved by "nudging" the states of the output nodes towards the correct prediction. This change than propagates throughout the network and weight updates can be computed from the two states of a given node during the evaluation and the training phase.
		
		\section{EqPropNEAT}
		Our method substitutes BP with equilibrium propagation. During the mutation stage of each generation, instead of assigning random weights to some connections, we employ equilibrium propagation iterations in order to adjust edge weights.
		
		As the network's complexity increases with more and more generations, the search space for weights increases rapidly. In order to get the most out of each network architecture proposed by the NEAT algorithm, we suggest adjusting the number of equilibrium propagation steps run between generations according to network complexity. Note that this means that the number of iterations varies even between species of the same generation. During the evolution process, networks only change gradually between each generation. This allows us to split the weight learning process between generations, so that the number of \eqprop iterations does not have to be enough for convergence as the training process will continue in the next generation. Too many \eqprop iterations could also cause overfitting which limits the network's ability to generalize, It even might limit the adaptability of species in future generations. 
		
		We have tested a number of different formulae for determining the number of necessary simulation iterations:
		
		\begin{align}
			N_1 &= V\\
			N_2 &= V+E\\
			N_3 &= V\cdot E
		\end{align}
		
		Where $V$ and $E$ denote the number of nodes and edges respectively. 
		
		TODO: fake figure about network complexity and needed iterations
		
		\subsection{Hardware acceleration}
		As mentioned in the introduction, \eqprop was designed with effective hardware implementations in mind. It has been shown \cite{hardware_eq}, that the \eqprop training of purely analog neural networks is indeed, possible. The missing piece for accelerating most of the EqPropNEAT stages is the capability to dynamically create the analog neural network representations for the current population. This would allow the use of hardware implemented \eqprop for high-speed training. 
		
		Fortunately, field-programmable analog arrays (FPAA) provide an off the shelf solution for reprogramming the interconnects between different analog devices, enabling the creation of hardware based analog neural networks while our algorithm is running. However, implementing \eqprop requires the ability to efficiently measure voltages in our network, as well as additional circuitry for digital operations such as computing the gradient. Field-programmable gate arrays (FPGA) are a much more popular digital counterpart to FPAAs. For our use case, a mixture of both systems is needed. While such devices have been already proposed in technical literature \cite{fgbfpmsa}, to the best of our knowledge, they have never been manufactured in a meaningful quantity yet. In theory, a mostly hardware accelerated implementation of our algorithm will be possible, once the above mentioned hardware becomes widely available. Figure \ref{fig:hardware} shows an outline of this algorithm. 
		
		Although at first glance the proposed hardware acceleration seems to improve runtimes significantly, we must mention a couple of potential performance limiting bottlenecks. Genetic algorithms in general require a population with thousands of genoms to provide enough diversity. Without this the algorithm does not work, or at least needs a lot more generations to arrive at a solution. It is also even more susceptible to overfitting. However with thousands of neural networks, with each of them potentially having at least hundreds of nodes, the required circuitry becomes problematically large. There is a high chance that even if the required hardware becomes available, it probably will only be able to contain a fraction of tha population at a time. Of course this problem can be somewhat alleviated by splitting the population into groups which are evaluated and trained in separate passes (slower runtime), however this way the number of occasions when the FPMA hardware is reconfigured with the new networks grows. In the case of modern day FPGAs the time it takes to reconfigure the entire board is usually in the realm of seconds and we believe this might not change in the near future since in most use cases this is not considered a bottleneck. As a consequence it is best to avoid reconfiguration as much as possible. At the time of writing this paper it is impossible to tell how much of a limiting factor this will be.
		
		Another potential issue is the lifetime of the hypothetic hardware. Based on similar currently available boards, each reconfiguration lowers life expectancy significantly. %Nowadays a typical FPGA 
		
		TODO: slow reconfiguration, hardware lifetime, runtime partial reconfiguration
		\tikzstyle{cloud} = [rectangle, draw, fill=gray!20, text width=1cm, text centered, minimum height=1.5em, rounded corners]
		\begin{figure}
			\centering
			\begin{tikzpicture}[node distance = 1.5cm, auto]
				\node[cloud, text width=4cm](init){Initialize population and create analog networks on FPMA};
				\node[cloud, text width=4cm, below of=init](eval){Evaluate the genomes (in hardware)};
				\node[cloud, text width=4cm, below of=eval](fromfpma){Copy results back from FPMA};
				\node[cloud, text width=2cm, below of=fromfpma](cross){Crossover};
				\node[cloud, text width=4cm, below of=cross](mut){Mutations without random edge weight changing rule};
				\node[cloud, text width=4cm, below of=mut](tofpma2){Create analog networks on FPMA};
				\node[cloud, text width=4cm, below of=tofpma2](eqprop){Equilibrium propagation iterations (in hardware)};
				
				\coordinate (A) at ($(eval.east)+(1cm,0cm)$);
				\coordinate (B) at ($(eqprop.east)+(1cm,0cm)$);
				
				\draw[->, arrows = {-{Stealth[scale=1.5]}}](init)--(eval);
				%\draw[->, arrows = {-{Stealth[scale=1.5]}}](tofpma1)--(eval);
				\draw[->, arrows = {-{Stealth[scale=1.5]}}](eval)--(fromfpma);
				\draw[->, arrows = {-{Stealth[scale=1.5]}}](fromfpma)--(cross);
				\draw[->, arrows = {-{Stealth[scale=1.5]}}](cross)--(mut);
				\draw[->, arrows = {-{Stealth[scale=1.5]}}](mut)--(tofpma2);
				\draw[->, arrows = {-{Stealth[scale=1.5]}}](tofpma2)--(eqprop);
				\draw[->, arrows = {-{Stealth[scale=1.5]}}] (eqprop) -- (B) -- node [right, align=left] {Repeat until good enough\\ solution is found} (A) -- (eval);
			\end{tikzpicture}
			\caption{An outline of the hardware accelerated EqPropNEAT algorithm. Note that the evaluation and weight training of genomes is entirely accomplished on the hardware level.}
			\label{fig:hardware}
		\end{figure}
		\subsection{Biologically plausible algorithm}
		As one of the main features of this study is the creation of a biologically plausible neural network model. For the generation of the network we went with a well tested solution, namely: NEAT. For the fine tuning of weights the obvious solution would have been the BP which offers great performance and is well tested and widely used to solve various task. However because of the aforementioned criteria of having a biologically plausible algorithm BP is off the table. For this reason we choose EP which utilizes a very similar concept with one key difference: it is considered biologically plausible. EP manages to achieve this by using the same algorithm for both phases of training. It's easy to see why BP's usage of different algorithms for these phases leads to the fact that it is considered biologically implausible. This way it's possible for the model to accurately represent the neural network of a living creature.
		
		Of course this whole idea only makes sense if the performance remains at similar levels to those of conventional solutions. In general the performance of EP is subpar compared to BP due the unique restriction of only using one algorithm for both training phases. This problem can be mitigated by the aforementioned hardware acceleration technique. We will discuss the results further in the \textit{Performance} chapter. As we have proven the performance can be a match or even an improvement compared to contemporary solutions thus we are left with only the benefits and no downsides. Benefits being the reduced complexity due to only one function being used for training and as a consequence of this the biological plausibility.
		
		Considering the results and implications of EqPropNeat we should be able to accurately reconstruct a living creature's neural network and with the proper hyperparameters. In theory we would be able to create a neural network which would engage the neurons in the same place and in the same order as the biological network it was based on. This would get us as closer to understanding how the brain works and also would raise some ethical questions but that is beyond the scope of this paper and this is only in theory so far.
		\section{Performance}
		\section{Conclusions}
		
		%\begin{thebibliography}{1}
		\bibliographystyle{IEEEtran}
		\bibliography{references}
		
		%  \end{thebibliography}
	
\end{document}
