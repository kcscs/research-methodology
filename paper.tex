\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\newcommand{\com}[1]{}

\begin{document}

%\title{A Sample Article Using IEEEtran.cls\\ for IEEE Journals and Transactions}
\title{EqPropNEAT\\Extending NEAT with equlibrium propagation}

\author{Cs. Karikó, M. Kovács, Zs. Tornai
%\author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
        % <-this % stops a space
%\thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
}

% The paper headers
\markboth{ELTE Computer Science MSc - Research methodology course}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\IEEEpubid{0000--0000/00\$00.00~\copyright~2022 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
  Artificial neural networks have been employed succesfully to solve a great variety of tasks over the past decade. In most applications, network architecture is usually decided with a trial-and-error process, relying on empirical experience.
  The neuoroevolution of augmenting topologies algorithm (NEAT) \cite{neat} proposes a solution to finding well fitting network architectures to a given task, however for learning weights, it only uses a simple genetic mutation rule.

  This paper proposes an extension of NEAT with an additional learning step employing equilibrium propagation \cite{eqprop} for learning edge weights. We claim that the resulting novel technique needs less iterations for convergence, as weights are adjusted by a method which was specifically invented for such task. Our experimental results show a significant reduction in training times on common neural network benchmark tasks, while achieving similar prediction quality.
\end{abstract}

% \begin{abstract}
% This document describes the most common article elements and how to use the IEEEtran class with \LaTeX \ to produce files that are suitable for submission to the IEEE.  IEEEtran can produce conference, journal, and technical note (correspondence) papers with a suitable choice of class options. 
% \end{abstract}

% Opposed to the standard backpropagation approach, equlibrium propagation designed for easy and efficient hardware implementation enabling mich higher performance and faster iterations, which are even more important in the case of genetic optimization techniques.

\begin{IEEEkeywords}
neuroevolution, neat, equilibrium propagation.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{T}{he} process of designing neural network models always involves deciding on a particular network architecture. In order to avoid both overfitting as well as subpar prediction quality, the right amount of network complexitiy has to be found for a particular learning task. Over the years, a great amount of experience has accomulated regarding this decision, however these rules are more about empirical know-how, than proven theorems. 

\section{Background}
As mentioned in the previous chapter, the NEAT algorithm uses a simple genetic mutation rule for evolving edge weights. With a sufficiently large population this method can evolve correct connection weights, however this process usually requires lot of generations. A number of extensions have aready been proposed for improving weight search by incorporating backpropagation to the original algorithm \cite{neatsurvey}. Such attempts include Learning-NEAT \cite{lneat}, DeepNeat and CoDeepNeat \cite{codeepneat}. A common property of these techniques is that s limited number of training epochs are run during evaluation for adjusting edge weights and determining network performance. 

Equilibrium propagation \cite{eqprop} proposes a new approach to training neural networks. Unlike conventional artificial neural network models, equilibrium propagation models the network as a continous-time dynamic system. Training and evaluation is accomplished with a numerical simulation of changing energy states converging to an equilibrium. After the simulation has arrived at a fixed point, the results can be read from the state of output nodes. Training is achieved by "nudging" the states of the output nodes towards the correct prediction. This change than propagates throughout the network and weight updates can be computed from the two states of a given node during the evaluation and the training phase.

\section{EqPropNEAT}
Our method substitutes backpropagation with equilibrium propagation. During the mutation stage of each generation, instead of assigning random weights to some connections, we employ equilibrium propagation iterations in order to adjust edge weights. 

As the network's complexity increases with more and more generations, the search space for weights increases rapidly. In order to get the most out of each network architecture proposed by the NEAT algorithm, we suggest adjusting the number of equilibrium propagation steps run between generations according to network complexity. We have tested a number of different formulae for determining the number of necessary simulation iterations:

\begin{align}
  N_1 &= V\\
  N_2 &= V+E\\
  N_3 &= V\cdot E
\end{align}

Where $V$ and $E$ denote the number of nodes and edges respectively.

TODO: fake figure about network complexity and needed iterations

%\begin{thebibliography}{1}
  \bibliographystyle{IEEEtran}
  \bibliography{references}
  
%  \end{thebibliography}

\end{document}
